{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FACTS Grounding](https://kaggle.com/facts-leaderboard) is a novel benchmark from Google DeepMind and Google Research designed to evaluate the factual accuracy and grounding of AI models.\n",
    "\n",
    "**As a primer, please check out this [Examples Section](https://kaggle.com/facts-leaderboard/examples) before running this notebook!**\n",
    "\n",
    "This notebook will walk you through:\n",
    "\n",
    "1) **[Generating responses](https://kaggle.com/facts-leaderboard/examples#response_generation)** for examples\n",
    "\n",
    "2) **[Evaluating responses](https://kaggle.com/facts-leaderboard/examples#response_evaluation)** that were generated\n",
    "\n",
    "3) **[Ensembling evaluations](https://kaggle.com/facts-leaderboard/examples#ensembling)** to produce a score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Setup Keys\n",
    "\n",
    "You will need your own set of API keys from [Google AI Studio](https://aistudio.google.com/app/apikey), [OpenAI](https://platform.openai.com/api-keys), and [Anthropic](https://docs.anthropic.com/en/api/getting-started) to run this notebook.\n",
    "\n",
    "**Add your keys as Kaggle Secrets with the following names:**\n",
    "\n",
    "1) `GOOGLE_AIS_API_KEY`\n",
    "\n",
    "2) `OPENAI_API_KEY`\n",
    "\n",
    "3) `ANTHROPIC_API_KEY`\n",
    "\n",
    "\n",
    "### **WARNING:** This notebook will execute real API calls against these keys. Please check usage limits before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "google_ais_api_key = os.getenv(\"GOOGLE_AIS_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from google import genai\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "google_client = genai.Client(api_key=google_ais_api_key)\n",
    "openai_client = OpenAI(api_key=openai_api_key)\n",
    "anthropic_client = Anthropic(api_key=anthropic_api_key)\n",
    "\n",
    "# models = [\"gemini-1.5-pro\", \"gpt-4o\", \"claude-3-5-sonnet\"]\n",
    "# models = [\"Qwen/Qwen2.5-7B-Instruct\"]\n",
    "models = [\"Qwen/Qwen2.5-1.5B-Instruct\"]\n",
    "def generate_gemini(prompt, sys):\n",
    "    response = google_client.models.generate_content(model='gemini-1.5-pro-002', contents=f'{sys} {prompt}')\n",
    "    return response.text\n",
    "\n",
    "def generate_gpt(prompt, sys):\n",
    "    if len(sys) > 0:\n",
    "        completion = openai_client.chat.completions.create(\n",
    "          model=\"gpt-4o\",\n",
    "          messages=[{\"role\": \"system\", \"content\": sys}, {\"role\": \"user\",\"content\": prompt}]\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    else:\n",
    "        completion = openai_client.chat.completions.create(\n",
    "          model=\"gpt-4o\",\n",
    "          messages=[{\"role\": \"user\",\"content\": prompt}]\n",
    "        )\n",
    "        return completion.choices[0].message.content     \n",
    "\n",
    "def generate_claude(prompt, sys):\n",
    "    if len(sys) > 0:\n",
    "        message = anthropic_client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=8192,\n",
    "            system=[{\"type\": \"text\", \"text\": sys}],\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "        return message.content[0].text\n",
    "    else:\n",
    "        message = anthropic_client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=8192,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "        return message.content[0].text\n",
    "\n",
    "def generate_huggingface(prompt, sys, model_name=\"gpt2\"):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    # Concatenate system and prompt if sys is not empty\n",
    "    if len(sys) > 0:\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": sys})\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    # Tokenize input\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    # Generate output\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=8192\n",
    "    )\n",
    "    # Decode and return\n",
    "\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "def generate(model, prompt, sys):\n",
    "    if model == \"gemini-1.5-pro\":\n",
    "        return generate_gemini(prompt, sys)\n",
    "    elif model == \"gpt-4o\":\n",
    "        return generate_gpt(prompt, sys)\n",
    "    elif model == \"claude-3-5-sonnet\":\n",
    "        return generate_claude(prompt, sys)\n",
    "    elif model == \"Qwen/Qwen2.5-1.5B-Instruct\":\n",
    "        return generate_huggingface(prompt, sys, model_name=model)\n",
    "    else:\n",
    "        raise Exception(\"Invalid model selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5311a5d0f5cf4df7810a3aaa548e2059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e159b9620ff4f1c9ff4c5397ee30e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca37d478dda4ce1b4a6d907a6334437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfb1b91b1f8456d8b4b8a0499dab03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0675885f150740dda19124aaf96a7311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9519143cc4a0419eb35b9194ead14c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4926eb1ea7134e59bba7f37f7afc4114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's the count to 3:\n",
      "\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Test client helper functions\n",
    "\n",
    "for model in models:\n",
    "    print(generate(model, \"count to 3\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Generation\n",
    "---\n",
    "\n",
    "We'll be using a subset of the examples from the [FACTS Grounding 1.0 Public Examples](https://kaggle.com/datasets/deepmind/FACTS-grounding-examples/data) dataset. This dataset contains 860 public examples (out of a total 1,719 examples) for the general public to access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system_instruction</th>\n",
       "      <th>user_request</th>\n",
       "      <th>context_document</th>\n",
       "      <th>full_prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[question]\\n [user request]\\n \\n\\n ===========...</td>\n",
       "      <td>Is the Government of the United States legally...</td>\n",
       "      <td>Passed following an address by Ukraine’s leade...</td>\n",
       "      <td>[question]\\n Is the Government of the United S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are required to use only information provi...</td>\n",
       "      <td>Explain the responsibilities of each departmen...</td>\n",
       "      <td>Procedural History In 2007, Mr. Taylor filed a...</td>\n",
       "      <td>Explain the responsibilities of each departmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Answer the question based solely on the inform...</td>\n",
       "      <td>Researchers at Foch Hospital in France publish...</td>\n",
       "      <td>Objectives: Maternal age has been increasing f...</td>\n",
       "      <td>Answer the question based solely on the inform...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  system_instruction  \\\n",
       "0  [question]\\n [user request]\\n \\n\\n ===========...   \n",
       "1  You are required to use only information provi...   \n",
       "2  Answer the question based solely on the inform...   \n",
       "\n",
       "                                        user_request  \\\n",
       "0  Is the Government of the United States legally...   \n",
       "1  Explain the responsibilities of each departmen...   \n",
       "2  Researchers at Foch Hospital in France publish...   \n",
       "\n",
       "                                    context_document  \\\n",
       "0  Passed following an address by Ukraine’s leade...   \n",
       "1  Procedural History In 2007, Mr. Taylor filed a...   \n",
       "2  Objectives: Maternal age has been increasing f...   \n",
       "\n",
       "                                         full_prompt  \n",
       "0  [question]\\n Is the Government of the United S...  \n",
       "1  Explain the responsibilities of each departmen...  \n",
       "2  Answer the question based solely on the inform...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "examples = pd.read_json(\"data/test.jsonl\", lines=True)\n",
    "examples.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                             | 1/430 [02:11<15:41:00, 131.61s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m     full_prompt = row[\u001b[33m'\u001b[39m\u001b[33mfull_prompt\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m         response = \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m         responses.loc[ix, [\u001b[33m\"\u001b[39m\u001b[33msystem_instruction\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33muser_request\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontext_document\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-response\u001b[39m\u001b[33m'\u001b[39m]] = [row[\u001b[33m\"\u001b[39m\u001b[33msystem_instruction\u001b[39m\u001b[33m\"\u001b[39m], row[\u001b[33m\"\u001b[39m\u001b[33muser_request\u001b[39m\u001b[33m\"\u001b[39m], row[\u001b[33m\"\u001b[39m\u001b[33mcontext_document\u001b[39m\u001b[33m\"\u001b[39m], response]\n\u001b[32m     10\u001b[39m responses\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mgenerate\u001b[39m\u001b[34m(model, prompt, sys)\u001b[39m\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m generate_claude(prompt, sys)\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m model == \u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen2.5-1.5B-Instruct\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgenerate_huggingface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInvalid model selected\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mgenerate_huggingface\u001b[39m\u001b[34m(prompt, sys, model_name)\u001b[39m\n\u001b[32m     70\u001b[39m model_inputs = tokenizer([text], return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(device)\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Generate output\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m generated_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8192\u001b[39;49m\n\u001b[32m     75\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Decode and return\u001b[39;00m\n\u001b[32m     78\u001b[39m generated_ids = [\n\u001b[32m     79\u001b[39m     output_ids[\u001b[38;5;28mlen\u001b[39m(input_ids):] \u001b[38;5;28;01mfor\u001b[39;00m input_ids, output_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(model_inputs.input_ids, generated_ids)\n\u001b[32m     80\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/research/rl-grounding-llm/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/research/rl-grounding-llm/venv/lib/python3.11/site-packages/transformers/generation/utils.py:2465\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2457\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2458\u001b[39m         input_ids=input_ids,\n\u001b[32m   2459\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2460\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2461\u001b[39m         **model_kwargs,\n\u001b[32m   2462\u001b[39m     )\n\u001b[32m   2464\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2465\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2470\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2472\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2476\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2477\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2478\u001b[39m         input_ids=input_ids,\n\u001b[32m   2479\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2480\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2481\u001b[39m         **model_kwargs,\n\u001b[32m   2482\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/research/rl-grounding-llm/venv/lib/python3.11/site-packages/transformers/generation/utils.py:3434\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3432\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3434\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3436\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3437\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3438\u001b[39m     outputs,\n\u001b[32m   3439\u001b[39m     model_kwargs,\n\u001b[32m   3440\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3441\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/research/rl-grounding-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/research/rl-grounding-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/research/rl-grounding-llm/venv/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/research/rl-grounding-llm/venv/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/research/rl-grounding-llm/venv/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:823\u001b[39m, in \u001b[36mQwen2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    818\u001b[39m output_hidden_states = (\n\u001b[32m    819\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    820\u001b[39m )\n\u001b[32m    822\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    836\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    837\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/research/rl-grounding-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/research/rl-grounding-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/research/rl-grounding-llm/venv/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/research/rl-grounding-llm/venv/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:549\u001b[39m, in \u001b[36mQwen2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    537\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    538\u001b[39m         partial(decoder_layer.\u001b[34m__call__\u001b[39m, **flash_attn_kwargs),\n\u001b[32m    539\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    546\u001b[39m         position_embeddings,\n\u001b[32m    547\u001b[39m     )\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/research/rl-grounding-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/research/rl-grounding-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/research/rl-grounding-llm/venv/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:262\u001b[39m, in \u001b[36mQwen2DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    261\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    275\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/research/rl-grounding-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/research/rl-grounding-llm/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/research/rl-grounding-llm/venv/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:194\u001b[39m, in \u001b[36mQwen2Attention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    192\u001b[39m         attention_interface = ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m.config._attn_implementation]\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m attn_output, attn_weights = \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43msliding_window\u001b[49m\u001b[43m=\u001b[49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# main diff with Llama\u001b[39;49;00m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m attn_output = attn_output.reshape(*input_shape, -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m    207\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.o_proj(attn_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/research/rl-grounding-llm/venv/lib/python3.11/site-packages/transformers/integrations/sdpa_attention.py:54\u001b[39m, in \u001b[36msdpa_attention_forward\u001b[39m\u001b[34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.jit.is_tracing() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(is_causal, torch.Tensor):\n\u001b[32m     52\u001b[39m     is_causal = is_causal.item()\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous()\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "responses = pd.DataFrame()\n",
    "for ix, row in tqdm(examples.iterrows(), total=len(examples)):\n",
    "    full_prompt = row['full_prompt']\n",
    "    for model in models:\n",
    "        response = generate(model=model, prompt=full_prompt, sys=\"\")\n",
    "        responses.loc[ix, [\"system_instruction\", \"user_request\", \"context_document\", f'{model}-response']] = [row[\"system_instruction\"], row[\"user_request\"], row[\"context_document\"], response]\n",
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system_instruction</th>\n",
       "      <th>user_request</th>\n",
       "      <th>context_document</th>\n",
       "      <th>Qwen/Qwen2.5-1.5B-Instruct-response</th>\n",
       "      <th>Qwen/Qwen2.5-1.5B-Instruct-response-length</th>\n",
       "      <th>average-response-length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[question]\\n [user request]\\n \\n\\n ===========...</td>\n",
       "      <td>Is the Government of the United States legally...</td>\n",
       "      <td>Passed following an address by Ukraine’s leade...</td>\n",
       "      <td>Yes, the Government of the United States is le...</td>\n",
       "      <td>598</td>\n",
       "      <td>598.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  system_instruction  \\\n",
       "0  [question]\\n [user request]\\n \\n\\n ===========...   \n",
       "\n",
       "                                        user_request  \\\n",
       "0  Is the Government of the United States legally...   \n",
       "\n",
       "                                    context_document  \\\n",
       "0  Passed following an address by Ukraine’s leade...   \n",
       "\n",
       "                 Qwen/Qwen2.5-1.5B-Instruct-response  \\\n",
       "0  Yes, the Government of the United States is le...   \n",
       "\n",
       "   Qwen/Qwen2.5-1.5B-Instruct-response-length  average-response-length  \n",
       "0                                         598                    598.0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for model in models:\n",
    "    responses[f'{model}-response-length'] = responses[f'{model}-response'].str.len()\n",
    "responses[\"average-response-length\"] = responses[[f'{model}-response-length' for model in models]].mean(axis=1)\n",
    "\n",
    "responses.sort_values(\"average-response-length\", inplace=True)\n",
    "responses.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses saved to data/responses/model1_responses.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "responses_dir = os.path.join(\"data\", \"responses\")\n",
    "os.makedirs(responses_dir, exist_ok=True)\n",
    "\n",
    "responses_file = os.path.join(responses_dir, \"model1_responses.csv\")\n",
    "responses.to_csv(responses_file, index=True)\n",
    "print(f\"Responses saved to {responses_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>system_instruction</th>\n",
       "      <th>user_request</th>\n",
       "      <th>context_document</th>\n",
       "      <th>Qwen/Qwen2.5-7B-Instruct-response</th>\n",
       "      <th>Qwen/Qwen2.5-7B-Instruct-response-length</th>\n",
       "      <th>average-response-length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112</td>\n",
       "      <td>Answer in 10 words or less. Keep things simple...</td>\n",
       "      <td>Give me a list of people that died in 1957.</td>\n",
       "      <td>After the show, Christian Dior began thinking ...</td>\n",
       "      <td>Sorry, no specific deaths in 1957 mentioned.</td>\n",
       "      <td>44</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>314</td>\n",
       "      <td>Give me your answer as a full sentence. Answer...</td>\n",
       "      <td>According to this transcript, what was ipad re...</td>\n",
       "      <td>**Tim Cook**\\n\\nThank you. Suhasini. Good afte...</td>\n",
       "      <td>iPad revenue in the December quarter was $7 bi...</td>\n",
       "      <td>52</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>292</td>\n",
       "      <td>Use only the provided context for your respons...</td>\n",
       "      <td>How much does a pair of Diamond 210 cost?</td>\n",
       "      <td>Title: Wharfedale Diamonds Shine Even Brighter...</td>\n",
       "      <td>According to the document, a pair of Diamond 2...</td>\n",
       "      <td>63</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>270</td>\n",
       "      <td>Your response to the user must only use the in...</td>\n",
       "      <td>Which model showed debt underutilization in th...</td>\n",
       "      <td>Since Modigliani and Miller (1958), economists...</td>\n",
       "      <td>- The multistage model showed debt underutiliz...</td>\n",
       "      <td>66</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>346</td>\n",
       "      <td>Answer in complete sentences, only use the con...</td>\n",
       "      <td>According to the document, how many copies of ...</td>\n",
       "      <td>**What's publicly known about \"Switch 2\"**\\n\\n...</td>\n",
       "      <td>According to the document, Mario Kart 8 Deluxe...</td>\n",
       "      <td>74</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                 system_instruction  \\\n",
       "0         112  Answer in 10 words or less. Keep things simple...   \n",
       "1         314  Give me your answer as a full sentence. Answer...   \n",
       "2         292  Use only the provided context for your respons...   \n",
       "3         270  Your response to the user must only use the in...   \n",
       "4         346  Answer in complete sentences, only use the con...   \n",
       "\n",
       "                                        user_request  \\\n",
       "0        Give me a list of people that died in 1957.   \n",
       "1  According to this transcript, what was ipad re...   \n",
       "2          How much does a pair of Diamond 210 cost?   \n",
       "3  Which model showed debt underutilization in th...   \n",
       "4  According to the document, how many copies of ...   \n",
       "\n",
       "                                    context_document  \\\n",
       "0  After the show, Christian Dior began thinking ...   \n",
       "1  **Tim Cook**\\n\\nThank you. Suhasini. Good afte...   \n",
       "2  Title: Wharfedale Diamonds Shine Even Brighter...   \n",
       "3  Since Modigliani and Miller (1958), economists...   \n",
       "4  **What's publicly known about \"Switch 2\"**\\n\\n...   \n",
       "\n",
       "                   Qwen/Qwen2.5-7B-Instruct-response  \\\n",
       "0       Sorry, no specific deaths in 1957 mentioned.   \n",
       "1  iPad revenue in the December quarter was $7 bi...   \n",
       "2  According to the document, a pair of Diamond 2...   \n",
       "3  - The multistage model showed debt underutiliz...   \n",
       "4  According to the document, Mario Kart 8 Deluxe...   \n",
       "\n",
       "   Qwen/Qwen2.5-7B-Instruct-response-length  average-response-length  \n",
       "0                                        44                     44.0  \n",
       "1                                        52                     52.0  \n",
       "2                                        63                     63.0  \n",
       "3                                        66                     66.0  \n",
       "4                                        74                     74.0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "responses = pd.read_csv(\"data/responses/qwen_responses.csv\")\n",
    "responses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Evaluation Prompts\n",
    "\n",
    "##### Evaluation Prompt Selection\n",
    "\n",
    "We chose grounding and evaluation prompts based on evaluation against an internal set of human rated responses. See below for which evaluation prompt were selected and the [Technical Report](https://arxiv.org/abs/2501.03200) for more details.\n",
    "\n",
    "#### Grounding\n",
    "\n",
    "1) `Gemini-1.5-pro    : json`\n",
    "\n",
    "2) `GPT-4o            : json`\n",
    "\n",
    "3) `Claude-3-5-sonnet : implicit_span_level`\n",
    "\n",
    "#### Quality\n",
    "\n",
    "1) `Gemini-1.5-pro    : ineligible_responses_filter_no_context`\n",
    "\n",
    "2) `GPT-4o            : ineligible_responses_filter_no_context`\n",
    "\n",
    "3) `Claude-3-5-sonnet : ineligible_responses_filter_no_context`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T16:40:26.115445Z",
     "iopub.status.busy": "2025-01-07T16:40:26.115040Z",
     "iopub.status.idle": "2025-01-07T16:40:26.140989Z",
     "shell.execute_reply": "2025-01-07T16:40:26.139726Z",
     "shell.execute_reply.started": "2025-01-07T16:40:26.115415Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evaluation_method</th>\n",
       "      <th>evaluation_prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>response_level</td>\n",
       "      <td>Your task is to check if the Response is accur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>json_alt</td>\n",
       "      <td>You are a helpful and harmless AI assistant. Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>json</td>\n",
       "      <td>You are a helpful and harmless AI assistant. Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>json_with_double_check</td>\n",
       "      <td>Your task is to verify whether a given sentenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>span_level</td>\n",
       "      <td>Your task is to check if a specific Span is ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>implicit_span_level</td>\n",
       "      <td>Your task is to check if the Response is accur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ineligible_responses_filter_with_context</td>\n",
       "      <td>Your mission is to judge the response from an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ineligible_responses_filter_no_context</td>\n",
       "      <td>Your mission is to judge the response from an ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          evaluation_method  \\\n",
       "0                            response_level   \n",
       "1                                  json_alt   \n",
       "2                                      json   \n",
       "3                    json_with_double_check   \n",
       "4                                span_level   \n",
       "5                       implicit_span_level   \n",
       "6  ineligible_responses_filter_with_context   \n",
       "7    ineligible_responses_filter_no_context   \n",
       "\n",
       "                                   evaluation_prompt  \n",
       "0  Your task is to check if the Response is accur...  \n",
       "1  You are a helpful and harmless AI assistant. Y...  \n",
       "2  You are a helpful and harmless AI assistant. Y...  \n",
       "3  Your task is to verify whether a given sentenc...  \n",
       "4  Your task is to check if a specific Span is ac...  \n",
       "5  Your task is to check if the Response is accur...  \n",
       "6  Your mission is to judge the response from an ...  \n",
       "7  Your mission is to judge the response from an ...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_prompts = pd.read_csv(\"/kaggle/input/FACTS-grounding-examples/evaluation_prompts.csv\")\n",
    "evaluation_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T16:40:26.326006Z",
     "iopub.status.busy": "2025-01-07T16:40:26.325586Z",
     "iopub.status.idle": "2025-01-07T16:40:26.335301Z",
     "shell.execute_reply": "2025-01-07T16:40:26.333927Z",
     "shell.execute_reply.started": "2025-01-07T16:40:26.325971Z"
    }
   },
   "outputs": [],
   "source": [
    "json_prompt = evaluation_prompts.loc[evaluation_prompts[\"evaluation_method\"] == 'json', \"evaluation_prompt\"].values[0]\n",
    "implicit_span_prompt = evaluation_prompts.loc[evaluation_prompts[\"evaluation_method\"] == 'implicit_span_level', \"evaluation_prompt\"].values[0]\n",
    "ineligible_responses_filter_no_context_prompt = evaluation_prompts.loc[evaluation_prompts[\"evaluation_method\"] == 'ineligible_responses_filter_with_context', \"evaluation_prompt\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Evaluation Helpers\n",
    "\n",
    "Here we'll create a helper method for each evaluation type. Note that, for grounding, the Gemini-1.5-pro and GPT-4o judges use the `json` evaluation prompt and that the Claude-3-5-sonnet judge uses the `implicit_span_level` evaluation prompt as described above.\n",
    "\n",
    "#### Testing Grounding Judges\n",
    "We'll test grounding with a situation where the response is correct but not grounded. We expect to see a `False` from all judges!\n",
    "\n",
    "```\n",
    "user_request:      \"what is 2 + 2?\"\n",
    "context_document:  \"2 + 2 is 3\"\n",
    "response:          \"2 + 2 is 4\" (this is correct but not grounded in the context_document)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T16:40:27.771151Z",
     "iopub.status.busy": "2025-01-07T16:40:27.770705Z",
     "iopub.status.idle": "2025-01-07T16:40:27.781722Z",
     "shell.execute_reply": "2025-01-07T16:40:27.780588Z",
     "shell.execute_reply.started": "2025-01-07T16:40:27.771114Z"
    }
   },
   "outputs": [],
   "source": [
    "# Grounding json evaluation\n",
    "\n",
    "import json\n",
    "\n",
    "def parse_structured_json(ans):\n",
    "  if '```json' in ans:\n",
    "      ans = ans.split('```json')[1].split('```')[0]\n",
    "  ans = ans.strip()\n",
    "  ans = ans.replace('}\\n', '}\\n@\\n@\\n')\n",
    "  parsed_answers = []\n",
    "  for line in ans.split('\\n@\\n@\\n'):\n",
    "    try:\n",
    "      line = line.replace('\\n', ' ')\n",
    "      line = line.replace(\"\\\\'\", \"'\")\n",
    "      parsed = json.loads(line)\n",
    "      parsed_answers.append(parsed)\n",
    "    except:\n",
    "      pass\n",
    "  if len(parsed_answers) > 0:\n",
    "    bool_ans = all(d['label'] == 'supported' or d['label'] == 'no_rad' for d in parsed_answers)\n",
    "  else:\n",
    "    bool_ans = False\n",
    "  return bool_ans, parsed_answers\n",
    "\n",
    "def evaluate_grounding_json(user_request, context_document, response, model):\n",
    "    prompt = json_prompt.replace('{{user_request}}', user_request).replace('{{context_document}}', context_document).replace('{{response}}', response)\n",
    "\n",
    "    evaluation_text = generate(model=model, prompt=prompt, sys=\"\")\n",
    "    evaluation, parsed = parse_structured_json(evaluation_text)\n",
    "    return evaluation\n",
    "\n",
    "def evaluate_grounding_gemini(user_request, context_document, response):\n",
    "    return evaluate_grounding_json(user_request, context_document, response, model=\"gemini-1.5-pro\")\n",
    "\n",
    "def evaluate_grounding_gpt(user_request, context_document, response):\n",
    "    return evaluate_grounding_json(user_request, context_document, response, model=\"gpt-4o\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T16:40:28.940284Z",
     "iopub.status.busy": "2025-01-07T16:40:28.939844Z",
     "iopub.status.idle": "2025-01-07T16:40:34.454350Z",
     "shell.execute_reply": "2025-01-07T16:40:34.453137Z",
     "shell.execute_reply.started": "2025-01-07T16:40:28.940247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Test the evaluator\n",
    "print(evaluate_grounding_gemini(user_request=\"what is 2 + 2?\", context_document=\"2 + 2 is 3\", response=\"2 + 2 is 4\"))\n",
    "print(evaluate_grounding_gpt(user_request=\"what is 2 + 2?\", context_document=\"2 + 2 is 3\", response=\"2 + 2 is 4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T16:40:34.456971Z",
     "iopub.status.busy": "2025-01-07T16:40:34.456488Z",
     "iopub.status.idle": "2025-01-07T16:40:34.465830Z",
     "shell.execute_reply": "2025-01-07T16:40:34.464560Z",
     "shell.execute_reply.started": "2025-01-07T16:40:34.456925Z"
    }
   },
   "outputs": [],
   "source": [
    "# Grounding implicit_span_level evaluation\n",
    "\n",
    "def answer_normalization(answer):\n",
    "  answer = answer.strip().lower()\n",
    "  if 'inaccurate' in answer or 'false' in answer:\n",
    "    return False\n",
    "  elif 'accurate' in answer or 'true' in answer:\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "    \n",
    "def evaluate_grounding_implicit_span(user_request, context_document, response, model):\n",
    "    prompt = implicit_span_prompt.replace('{{user_request}}', user_request).replace('{{context_document}}', context_document).replace('{{response}}', response)\n",
    "    \n",
    "    evaluation_text = generate(model=model, prompt=prompt, sys=\"\")\n",
    "    splits = evaluation_text.split('Final Answer:')\n",
    "    if (len(splits) <= 1):\n",
    "        return False\n",
    "    final_ans = splits[1]\n",
    "    return answer_normalization(final_ans)\n",
    "\n",
    "def evaluate_grounding_claude(user_request, context_document, response):\n",
    "    return evaluate_grounding_implicit_span(user_request, context_document, response, model=\"claude-3-5-sonnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T16:40:34.467587Z",
     "iopub.status.busy": "2025-01-07T16:40:34.467224Z",
     "iopub.status.idle": "2025-01-07T16:40:37.158644Z",
     "shell.execute_reply": "2025-01-07T16:40:37.157479Z",
     "shell.execute_reply.started": "2025-01-07T16:40:34.467532Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the evaluator\n",
    "evaluate_grounding_claude(user_request=\"what is 2 + 2?\", context_document=\"2 + 2 is 3\", response=\"2 + 2 is 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Quality Judges\n",
    "\n",
    "We'll test quality with a situation where the response does not answer the posed request. We expect to see a `False` from all judges!\n",
    "\n",
    "```\n",
    "user_request:      \"what is 2 + 2?\"\n",
    "response_a:        \"3 + 3 is 6\" (this is the response we evaluate)\n",
    "response_b:        \"2 + 2 is 4\" (this is the reference response)\n",
    "```\n",
    "\n",
    "We compare our response with a reference because it improves the accuracy of the judges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T16:40:37.161902Z",
     "iopub.status.busy": "2025-01-07T16:40:37.161418Z",
     "iopub.status.idle": "2025-01-07T16:40:37.173881Z",
     "shell.execute_reply": "2025-01-07T16:40:37.172579Z",
     "shell.execute_reply.started": "2025-01-07T16:40:37.161853Z"
    }
   },
   "outputs": [],
   "source": [
    "# Quality no_context evaluation\n",
    "\n",
    "QUESTIONS_TO_LABELS = {\n",
    "    'Instruction Following': ['No Issues', 'Minor Issue(s)', 'Major Issue(s)', 'Invalid'],\n",
    "}\n",
    "\n",
    "def parse_json(ans):\n",
    "    parsed = {}\n",
    "    if '```json' in ans:\n",
    "        ans = ans.split('```json')[1]\n",
    "        ans = ans.split('```')[0]\n",
    "    ans = ans.replace('\\n', ' ')\n",
    "    try:\n",
    "        parsed = json.loads(ans)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    if 'Instruction Following' not in parsed:\n",
    "        parsed['Instruction Following'] = 'Invalid'\n",
    "    elif parsed['Instruction Following'] not in ['No Issues', 'Minor Issue(s)', 'Major Issue(s)', 'Invalid']:\n",
    "        parsed['Instruction Following'] = 'Invalid'\n",
    "    return parsed\n",
    "\n",
    "# Evaluates response_a for quality using response_b as a reference.\n",
    "def evaluate_quality_no_context(user_request, response_a, response_b, model):\n",
    "    prompt = ineligible_responses_filter_no_context_prompt.replace('{{user_request}}', user_request).replace('{{response_a}}', response_a).replace('{{response_b}}', response_b)\n",
    "    \n",
    "    evaluation_text = generate(prompt=prompt, model=model, sys=\"\")\n",
    "    parsed = parse_json(evaluation_text)\n",
    "\n",
    "    return \"Major Issue(s)\" not in parsed['Instruction Following']\n",
    "\n",
    "# Use the response from the judge model itself as the reference response (response_b).\n",
    "def evaluate_quality_no_context_gemini(user_request, response, references):\n",
    "    return evaluate_quality_no_context(user_request, response, references[\"gemini-1.5-pro-response\"], model=\"gemini-1.5-pro\")\n",
    "\n",
    "def evaluate_quality_no_context_gpt(user_request, response, references):\n",
    "    return evaluate_quality_no_context(user_request, response, references[\"gpt-4o-response\"], model=\"gpt-4o\")\n",
    "\n",
    "def evaluate_quality_no_context_claude(user_request, response, references):\n",
    "    return evaluate_quality_no_context(user_request, response, references[\"claude-3-5-sonnet-response\"], model=\"claude-3-5-sonnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T16:40:37.175713Z",
     "iopub.status.busy": "2025-01-07T16:40:37.175348Z",
     "iopub.status.idle": "2025-01-07T16:41:07.795849Z",
     "shell.execute_reply": "2025-01-07T16:41:07.794540Z",
     "shell.execute_reply.started": "2025-01-07T16:40:37.175680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Example references dataframe, when we're actually running the evaluation we'll be using our generated responses from before as references.\n",
    "references = pd.DataFrame({\n",
    "    \"gemini-1.5-pro-response\": [\"2 + 2 is 4\"],\n",
    "    \"gpt-4o-response\": [\"2 + 2 is 4\"],\n",
    "    \"claude-3-5-sonnet-response\": [\"2 + 2 is 4\"],\n",
    "})\n",
    "\n",
    "print(evaluate_quality_no_context_gemini(\"What is 2 + 2?\", response=\"3 + 3 is 6\", references=references.loc[0]))\n",
    "print(evaluate_quality_no_context_gpt(\"What is 2 + 2?\", response=\"3 + 3 is 6\", references=references.loc[0]))\n",
    "print(evaluate_quality_no_context_claude(\"What is 2 + 2?\", response=\"3 + 3 is 6\", references=references.loc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Responses (~15 minutes)\n",
    "\n",
    "With our evaluation helper methods, we can now evaluate the responses we generated before.\n",
    "\n",
    "**Please re-run the below cell if it fails due to transient API issues.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T16:41:07.797705Z",
     "iopub.status.busy": "2025-01-07T16:41:07.797315Z",
     "iopub.status.idle": "2025-01-07T16:46:02.419422Z",
     "shell.execute_reply": "2025-01-07T16:46:02.418379Z",
     "shell.execute_reply.started": "2025-01-07T16:41:07.797671Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating response at ix: 17 for model gemini-1.5-pro with:\n",
      "    evaluate_grounding_gemini\n",
      "    evaluate_grounding_gpt\n",
      "    evaluate_grounding_claude\n",
      "    evaluate_quality_no_context_gemini\n",
      "    evaluate_quality_no_context_gpt\n",
      "    evaluate_quality_no_context_claude\n",
      "Evaluating response at ix: 17 for model gpt-4o with:\n",
      "    evaluate_grounding_gemini\n",
      "    evaluate_grounding_gpt\n",
      "    evaluate_grounding_claude\n",
      "    evaluate_quality_no_context_gemini\n",
      "    evaluate_quality_no_context_gpt\n",
      "    evaluate_quality_no_context_claude\n",
      "Evaluating response at ix: 17 for model claude-3-5-sonnet with:\n",
      "    evaluate_grounding_gemini\n",
      "    evaluate_grounding_gpt\n",
      "    evaluate_grounding_claude\n",
      "    evaluate_quality_no_context_gemini\n",
      "    evaluate_quality_no_context_gpt\n",
      "    evaluate_quality_no_context_claude\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [01:25<02:50, 85.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating response at ix: 6 for model gemini-1.5-pro with:\n",
      "    evaluate_grounding_gemini\n",
      "    evaluate_grounding_gpt\n",
      "    evaluate_grounding_claude\n",
      "    evaluate_quality_no_context_gemini\n",
      "    evaluate_quality_no_context_gpt\n",
      "    evaluate_quality_no_context_claude\n",
      "Evaluating response at ix: 6 for model gpt-4o with:\n",
      "    evaluate_grounding_gemini\n",
      "    evaluate_grounding_gpt\n",
      "    evaluate_grounding_claude\n",
      "    evaluate_quality_no_context_gemini\n",
      "    evaluate_quality_no_context_gpt\n",
      "    evaluate_quality_no_context_claude\n",
      "Evaluating response at ix: 6 for model claude-3-5-sonnet with:\n",
      "    evaluate_grounding_gemini\n",
      "    evaluate_grounding_gpt\n",
      "    evaluate_grounding_claude\n",
      "    evaluate_quality_no_context_gemini\n",
      "    evaluate_quality_no_context_gpt\n",
      "    evaluate_quality_no_context_claude\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [03:10<01:36, 96.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating response at ix: 15 for model gemini-1.5-pro with:\n",
      "    evaluate_grounding_gemini\n",
      "    evaluate_grounding_gpt\n",
      "    evaluate_grounding_claude\n",
      "    evaluate_quality_no_context_gemini\n",
      "    evaluate_quality_no_context_gpt\n",
      "    evaluate_quality_no_context_claude\n",
      "Evaluating response at ix: 15 for model gpt-4o with:\n",
      "    evaluate_grounding_gemini\n",
      "    evaluate_grounding_gpt\n",
      "    evaluate_grounding_claude\n",
      "    evaluate_quality_no_context_gemini\n",
      "    evaluate_quality_no_context_gpt\n",
      "    evaluate_quality_no_context_claude\n",
      "Evaluating response at ix: 15 for model claude-3-5-sonnet with:\n",
      "    evaluate_grounding_gemini\n",
      "    evaluate_grounding_gpt\n",
      "    evaluate_grounding_claude\n",
      "    evaluate_quality_no_context_gemini\n",
      "    evaluate_quality_no_context_gpt\n",
      "    evaluate_quality_no_context_claude\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [04:54<00:00, 98.20s/it] \n"
     ]
    }
   ],
   "source": [
    "grounding_evaluation_methods = [\n",
    "    evaluate_grounding_gemini,\n",
    "    evaluate_grounding_gpt,\n",
    "    evaluate_grounding_claude,\n",
    "]\n",
    "\n",
    "quality_evaluation_methods = [\n",
    "    evaluate_quality_no_context_gemini,\n",
    "    evaluate_quality_no_context_gpt,\n",
    "    evaluate_quality_no_context_claude,\n",
    "]\n",
    "\n",
    "def nameof(f):\n",
    "    return f.__name__.replace(\"_\", \"-\")\n",
    "\n",
    "for ix, row in tqdm(responses.iterrows(), total=len(responses)):    \n",
    "    user_request = row[\"user_request\"]\n",
    "    context_document = row[\"context_document\"]\n",
    "    \n",
    "    for evaluated_model in models:\n",
    "        response_column_key = f'{evaluated_model}-response'\n",
    "        response = row[response_column_key]\n",
    "        \n",
    "        print(f'Evaluating response at ix: {ix} for model {evaluated_model} with:')\n",
    "        for eval_method in grounding_evaluation_methods:\n",
    "            print(f'    {eval_method.__name__}')\n",
    "            key = f'{response_column_key}-{nameof(eval_method)}'\n",
    "            \n",
    "            # skip rows if we've evaluated already\n",
    "            if ix in responses.index and key in responses.columns and pd.notna(responses.loc[ix, key]):\n",
    "                continue\n",
    "\n",
    "            evaluation = eval_method(user_request, context_document, response)\n",
    "            responses.loc[ix, [key]] = [evaluation]\n",
    "            \n",
    "        for eval_method in quality_evaluation_methods:\n",
    "            print(f'    {eval_method.__name__}')\n",
    "            key = f'{response_column_key}-{nameof(eval_method)}'\n",
    "\n",
    "            # skip rows if we've evaluated already\n",
    "            if ix in responses.index and key in responses.columns and pd.notna(responses.loc[ix, key]):\n",
    "                continue\n",
    "            \n",
    "            evaluation = eval_method(user_request, response, row)\n",
    "            responses.loc[ix,[key]] = [evaluation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T16:46:02.421998Z",
     "iopub.status.busy": "2025-01-07T16:46:02.421005Z",
     "iopub.status.idle": "2025-01-07T16:46:02.443365Z",
     "shell.execute_reply": "2025-01-07T16:46:02.442322Z",
     "shell.execute_reply.started": "2025-01-07T16:46:02.421949Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claude-3-5-sonnet-response-evaluate-grounding-claude</th>\n",
       "      <th>claude-3-5-sonnet-response-evaluate-grounding-gemini</th>\n",
       "      <th>claude-3-5-sonnet-response-evaluate-grounding-gpt</th>\n",
       "      <th>claude-3-5-sonnet-response-evaluate-quality-no-context-claude</th>\n",
       "      <th>claude-3-5-sonnet-response-evaluate-quality-no-context-gemini</th>\n",
       "      <th>claude-3-5-sonnet-response-evaluate-quality-no-context-gpt</th>\n",
       "      <th>gemini-1.5-pro-response-evaluate-grounding-claude</th>\n",
       "      <th>gemini-1.5-pro-response-evaluate-grounding-gemini</th>\n",
       "      <th>gemini-1.5-pro-response-evaluate-grounding-gpt</th>\n",
       "      <th>gemini-1.5-pro-response-evaluate-quality-no-context-claude</th>\n",
       "      <th>gemini-1.5-pro-response-evaluate-quality-no-context-gemini</th>\n",
       "      <th>gemini-1.5-pro-response-evaluate-quality-no-context-gpt</th>\n",
       "      <th>gpt-4o-response-evaluate-grounding-claude</th>\n",
       "      <th>gpt-4o-response-evaluate-grounding-gemini</th>\n",
       "      <th>gpt-4o-response-evaluate-grounding-gpt</th>\n",
       "      <th>gpt-4o-response-evaluate-quality-no-context-claude</th>\n",
       "      <th>gpt-4o-response-evaluate-quality-no-context-gemini</th>\n",
       "      <th>gpt-4o-response-evaluate-quality-no-context-gpt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   claude-3-5-sonnet-response-evaluate-grounding-claude  \\\n",
       "17                                               True     \n",
       "6                                                True     \n",
       "15                                               True     \n",
       "\n",
       "   claude-3-5-sonnet-response-evaluate-grounding-gemini  \\\n",
       "17                                               True     \n",
       "6                                                True     \n",
       "15                                               True     \n",
       "\n",
       "   claude-3-5-sonnet-response-evaluate-grounding-gpt  \\\n",
       "17                                              True   \n",
       "6                                               True   \n",
       "15                                             False   \n",
       "\n",
       "   claude-3-5-sonnet-response-evaluate-quality-no-context-claude  \\\n",
       "17                                              False              \n",
       "6                                               False              \n",
       "15                                              False              \n",
       "\n",
       "   claude-3-5-sonnet-response-evaluate-quality-no-context-gemini  \\\n",
       "17                                               True              \n",
       "6                                                True              \n",
       "15                                               True              \n",
       "\n",
       "   claude-3-5-sonnet-response-evaluate-quality-no-context-gpt  \\\n",
       "17                                               True           \n",
       "6                                                True           \n",
       "15                                               True           \n",
       "\n",
       "   gemini-1.5-pro-response-evaluate-grounding-claude  \\\n",
       "17                                              True   \n",
       "6                                               True   \n",
       "15                                             False   \n",
       "\n",
       "   gemini-1.5-pro-response-evaluate-grounding-gemini  \\\n",
       "17                                              True   \n",
       "6                                               True   \n",
       "15                                              True   \n",
       "\n",
       "   gemini-1.5-pro-response-evaluate-grounding-gpt  \\\n",
       "17                                          False   \n",
       "6                                            True   \n",
       "15                                           True   \n",
       "\n",
       "   gemini-1.5-pro-response-evaluate-quality-no-context-claude  \\\n",
       "17                                              False           \n",
       "6                                                True           \n",
       "15                                              False           \n",
       "\n",
       "   gemini-1.5-pro-response-evaluate-quality-no-context-gemini  \\\n",
       "17                                               True           \n",
       "6                                               False           \n",
       "15                                               True           \n",
       "\n",
       "   gemini-1.5-pro-response-evaluate-quality-no-context-gpt  \\\n",
       "17                                              False        \n",
       "6                                               False        \n",
       "15                                               True        \n",
       "\n",
       "   gpt-4o-response-evaluate-grounding-claude  \\\n",
       "17                                      True   \n",
       "6                                       True   \n",
       "15                                      True   \n",
       "\n",
       "   gpt-4o-response-evaluate-grounding-gemini  \\\n",
       "17                                      True   \n",
       "6                                       True   \n",
       "15                                      True   \n",
       "\n",
       "   gpt-4o-response-evaluate-grounding-gpt  \\\n",
       "17                                   True   \n",
       "6                                    True   \n",
       "15                                   True   \n",
       "\n",
       "   gpt-4o-response-evaluate-quality-no-context-claude  \\\n",
       "17                                               True   \n",
       "6                                               False   \n",
       "15                                              False   \n",
       "\n",
       "   gpt-4o-response-evaluate-quality-no-context-gemini  \\\n",
       "17                                               True   \n",
       "6                                                True   \n",
       "15                                              False   \n",
       "\n",
       "   gpt-4o-response-evaluate-quality-no-context-gpt  \n",
       "17                                            True  \n",
       "6                                             True  \n",
       "15                                            True  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations = responses[[col for col in responses.columns if \"evaluate\" in col]]\n",
    "evaluations = evaluations.reindex(sorted(evaluations.columns), axis=1)\n",
    "evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Ensembling\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Quality Evaluations\n",
    "\n",
    "We'll invalidate responses only if it fails the bar for all three quality judges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T16:46:02.445239Z",
     "iopub.status.busy": "2025-01-07T16:46:02.444789Z",
     "iopub.status.idle": "2025-01-07T16:46:02.479723Z",
     "shell.execute_reply": "2025-01-07T16:46:02.478689Z",
     "shell.execute_reply.started": "2025-01-07T16:46:02.445194Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claude-3-5-sonnet-response-evaluate-grounding-claude</th>\n",
       "      <th>claude-3-5-sonnet-response-evaluate-grounding-gemini</th>\n",
       "      <th>claude-3-5-sonnet-response-evaluate-grounding-gpt</th>\n",
       "      <th>claude-3-5-sonnet-response-passes-qc</th>\n",
       "      <th>gemini-1.5-pro-response-evaluate-grounding-claude</th>\n",
       "      <th>gemini-1.5-pro-response-evaluate-grounding-gemini</th>\n",
       "      <th>gemini-1.5-pro-response-evaluate-grounding-gpt</th>\n",
       "      <th>gemini-1.5-pro-response-passes-qc</th>\n",
       "      <th>gpt-4o-response-evaluate-grounding-claude</th>\n",
       "      <th>gpt-4o-response-evaluate-grounding-gemini</th>\n",
       "      <th>gpt-4o-response-evaluate-grounding-gpt</th>\n",
       "      <th>gpt-4o-response-passes-qc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   claude-3-5-sonnet-response-evaluate-grounding-claude  \\\n",
       "17                                               True     \n",
       "6                                                True     \n",
       "15                                               True     \n",
       "\n",
       "   claude-3-5-sonnet-response-evaluate-grounding-gemini  \\\n",
       "17                                               True     \n",
       "6                                                True     \n",
       "15                                               True     \n",
       "\n",
       "   claude-3-5-sonnet-response-evaluate-grounding-gpt  \\\n",
       "17                                              True   \n",
       "6                                               True   \n",
       "15                                             False   \n",
       "\n",
       "   claude-3-5-sonnet-response-passes-qc  \\\n",
       "17                                 True   \n",
       "6                                  True   \n",
       "15                                 True   \n",
       "\n",
       "   gemini-1.5-pro-response-evaluate-grounding-claude  \\\n",
       "17                                              True   \n",
       "6                                               True   \n",
       "15                                             False   \n",
       "\n",
       "   gemini-1.5-pro-response-evaluate-grounding-gemini  \\\n",
       "17                                              True   \n",
       "6                                               True   \n",
       "15                                              True   \n",
       "\n",
       "   gemini-1.5-pro-response-evaluate-grounding-gpt  \\\n",
       "17                                          False   \n",
       "6                                            True   \n",
       "15                                           True   \n",
       "\n",
       "   gemini-1.5-pro-response-passes-qc  \\\n",
       "17                              True   \n",
       "6                               True   \n",
       "15                              True   \n",
       "\n",
       "   gpt-4o-response-evaluate-grounding-claude  \\\n",
       "17                                      True   \n",
       "6                                       True   \n",
       "15                                      True   \n",
       "\n",
       "   gpt-4o-response-evaluate-grounding-gemini  \\\n",
       "17                                      True   \n",
       "6                                       True   \n",
       "15                                      True   \n",
       "\n",
       "   gpt-4o-response-evaluate-grounding-gpt gpt-4o-response-passes-qc  \n",
       "17                                   True                      True  \n",
       "6                                    True                      True  \n",
       "15                                   True                      True  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ix, row in evaluations.iterrows():\n",
    "    for model in models:\n",
    "        passes_qc_count = 0\n",
    "        for evaluation_method in quality_evaluation_methods:\n",
    "            key = f'{model}-response-{nameof(evaluation_method)}'\n",
    "            passes_qc_count += 1 if row[key] else 0\n",
    "\n",
    "        passes_qc = passes_qc_count > 0\n",
    "\n",
    "        evaluations.loc[ix, [f'{model}-response-passes-qc']] = [passes_qc]\n",
    "\n",
    "relevant_columns = [col for col in evaluations.columns if (\"evaluate-grounding\" in col or \"passes-qc\" in col)]\n",
    "\n",
    "grounding_evaluations = evaluations[relevant_columns]\n",
    "grounding_evaluations = grounding_evaluations.reindex(sorted(grounding_evaluations.columns), axis=1)\n",
    "grounding_evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather Grounding Evaluations\n",
    "\n",
    "Then we'll sum all the responses that were considered grounded (and that passed the quality check) across each of the three grounding judges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T16:46:02.483141Z",
     "iopub.status.busy": "2025-01-07T16:46:02.482539Z",
     "iopub.status.idle": "2025-01-07T16:46:02.519356Z",
     "shell.execute_reply": "2025-01-07T16:46:02.517997Z",
     "shell.execute_reply.started": "2025-01-07T16:46:02.483091Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>evaluation_method</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemini-1.5-pro</td>\n",
       "      <td>evaluate-grounding-gemini</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemini-1.5-pro</td>\n",
       "      <td>evaluate-grounding-gpt</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemini-1.5-pro</td>\n",
       "      <td>evaluate-grounding-claude</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>evaluate-grounding-gemini</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>evaluate-grounding-gpt</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>evaluate-grounding-claude</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>claude-3-5-sonnet</td>\n",
       "      <td>evaluate-grounding-gemini</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>claude-3-5-sonnet</td>\n",
       "      <td>evaluate-grounding-gpt</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>claude-3-5-sonnet</td>\n",
       "      <td>evaluate-grounding-claude</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model          evaluation_method     score\n",
       "0     gemini-1.5-pro  evaluate-grounding-gemini       1.0\n",
       "1     gemini-1.5-pro     evaluate-grounding-gpt  0.666667\n",
       "2     gemini-1.5-pro  evaluate-grounding-claude  0.666667\n",
       "3             gpt-4o  evaluate-grounding-gemini       1.0\n",
       "4             gpt-4o     evaluate-grounding-gpt       1.0\n",
       "5             gpt-4o  evaluate-grounding-claude       1.0\n",
       "6  claude-3-5-sonnet  evaluate-grounding-gemini       1.0\n",
       "7  claude-3-5-sonnet     evaluate-grounding-gpt  0.666667\n",
       "8  claude-3-5-sonnet  evaluate-grounding-claude       1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = pd.DataFrame(columns=['model', 'evaluation_method', 'score'])\n",
    "n = 3\n",
    "for model in models:\n",
    "    for evaluation_method in grounding_evaluation_methods:\n",
    "        grounding_key = f'{model}-response-{nameof(evaluation_method)}'\n",
    "        quality_key = f'{model}-response-passes-qc'\n",
    "\n",
    "        passed_qc_responses = grounding_evaluations[grounding_evaluations[quality_key] == True]\n",
    "        n_success = passed_qc_responses[grounding_key].astype(int).sum()\n",
    "        \n",
    "        scores.loc[len(scores), ['model', 'evaluation_method', 'score']] = [model, nameof(evaluation_method), n_success / n]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Results\n",
    "\n",
    "Finally, we can average across the three LLM judges to arrive at final scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T16:46:02.521118Z",
     "iopub.status.busy": "2025-01-07T16:46:02.520772Z",
     "iopub.status.idle": "2025-01-07T16:46:02.545840Z",
     "shell.execute_reply": "2025-01-07T16:46:02.544522Z",
     "shell.execute_reply.started": "2025-01-07T16:46:02.521069Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>claude-3-5-sonnet</th>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini-1.5-pro</th>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4o</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      score\n",
       "model                      \n",
       "claude-3-5-sonnet  0.888889\n",
       "gemini-1.5-pro     0.777778\n",
       "gpt-4o                  1.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.groupby(\"model\")[[\"score\"]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing Remarks\n",
    "---\n",
    "\n",
    "Thank you for following through this entire notebook! It's important to note that we've gone through a tiny subset of the examples here. For the real set of scores, please check out the [official leaderboard](https://kaggle.com/facts-leaderboard).\n",
    "\n",
    "Questions, comments, or issues? Please feel free to share your thoughts with us in the [discussion forum](https://kaggle.com/facts-leaderboard/discussion?sort=hotness)!"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6223703,
     "sourceId": 10230640,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
